{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcdd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from rdflib import Graph, URIRef, Namespace\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "from rdflib import Literal\n",
    "from rdflib.namespace import XSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e245425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte découpé avec succès en 9 blocs (paragraphes).\n",
      "Exemple du bloc 1 : Geoffrey Everest Hinton (born 6 December 1947) is an English Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013 he divides his time working for Google (Google Brain) and the University of Toronto. In 2017, he cofounded and became the Chief Scientific Advisor of the Vector Institute in Toronto.\n"
     ]
    }
   ],
   "source": [
    "# Import du texte\n",
    "\n",
    "with open(\"texte_a_analyser.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_source = f.read()\n",
    "\n",
    "# Découpage : on sépare par les sauts de ligne (\\n) et on enlève les vides\n",
    "blocs = [b.strip() for b in text_source.split('\\n') if b.strip()]\n",
    "\n",
    "print(f\"Texte découpé avec succès en {len(blocs)} blocs (paragraphes).\")\n",
    "print(f\"Exemple du bloc 1 : {blocs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157d56f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'extraction REBEL...\n",
      "\n",
      "RAW OUTPUT BLOC 1 : '<s><triplet> Everest Hinton <subj> 6 December 1947 <obj> date of birth <subj> computer scientist <obj> occupation <subj> artificial neural network <obj> field of work <subj> University of Toronto <obj> employer <triplet> artificial neural network <subj> cognitive psychologist <obj> part of</s>'\n",
      "Bloc 1 : 5 triplets extraits.\n",
      "RAW OUTPUT BLOC 2 : '<s><triplet> AlexNet <subj> Alex Krizhevsky <obj> discoverer or inventor <triplet> Alex Krizhevsky <subj> AlexNet <obj> notable work</s>'\n",
      "Bloc 2 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 3 : '<s><triplet> Christopher Longuet-Higgins <subj> University of Edinburgh <obj> employer</s>'\n",
      "Bloc 3 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 4 : '<s><triplet> Google <subj> DNNresearch Inc. <obj> subsidiary <triplet> DNNresearch Inc. <subj> Google <obj> parent organization</s>'\n",
      "Bloc 4 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 5 : '<s><triplet> neural network <subj> machine learning <obj> use</s>'\n",
      "Bloc 5 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 6 : '<s><triplet> David E. Rumelhart <subj> Carnegie Mellon University <obj> educated at</s>'\n",
      "Bloc 6 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 7 : '<s><triplet> Boltzmann machine <subj> David Ackley <obj> discoverer or inventor <subj> Terry Sejnowski <obj> discoverer or inventor</s>'\n",
      "Bloc 7 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 8 : '<s><triplet> open-access <subj> research papers <obj> subclass of</s>'\n",
      "Bloc 8 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 9 : '<s><triplet> Richard Zemel <subj> Brendan Frey <obj> student <triplet> Brendan Frey <subj> Richard Zemel <obj> student of</s>'\n",
      "Bloc 9 : 2 triplets extraits.\n",
      "\n",
      "Terminé ! Total de triplets trouvés : 17\n"
     ]
    }
   ],
   "source": [
    "# REBEL\n",
    "\n",
    "# 1. Chargement du pipeline REBEL\n",
    "# Le premier lancement téléchargera le modèle (~1.6 Go)\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "# 2. Fonction minimale pour parser la sortie (String -> Liste de Dictionnaires)\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    # on enlève juste <s>, <pad>, </s> mais on garde <triplet>, <subj>, <obj>\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "\n",
    "# 3. Exécution sur les blocs\n",
    "print(\"Démarrage de l'extraction REBEL...\\n\")\n",
    "all_triplets = []\n",
    "\n",
    "for i, bloc in enumerate(blocs):\n",
    "    # 1) génération : on veut les ids, pas le texte nettoyé\n",
    "    out = triplet_extractor(\n",
    "        bloc,\n",
    "        max_length=512,\n",
    "        num_beams=3,\n",
    "        return_tensors=True,\n",
    "        return_text=False\n",
    "    )[0][\"generated_token_ids\"]\n",
    "\n",
    "    # 2) décodage manuel en gardant les tokens spéciaux\n",
    "    raw_output = triplet_extractor.tokenizer.batch_decode(\n",
    "        [out],\n",
    "        skip_special_tokens=False\n",
    "    )[0]\n",
    "\n",
    "    # Debug si tu veux voir la chaîne brute\n",
    "    print(f\"RAW OUTPUT BLOC {i+1} : {repr(raw_output)}\")\n",
    "\n",
    "    # 3) extraction des triplets\n",
    "    triplets = extract_triplets(raw_output)\n",
    "    all_triplets.extend(triplets)\n",
    "\n",
    "    print(f\"Bloc {i+1} : {len(triplets)} triplets extraits.\")\n",
    "\n",
    "print(f\"\\nTerminé ! Total de triplets trouvés : {len(all_triplets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc0dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_uri(text_entity, confidence=0.5, support=20):\n",
    "    api_url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "    params = {\n",
    "        \"text\": text_entity,\n",
    "        \"confidence\": confidence,\n",
    "        \"support\": support\n",
    "    }\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(0.2) \n",
    "        response = requests.get(api_url, params=params, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'Resources' in data:\n",
    "                return data['Resources'][0]['@URI']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_literal_if_date_or_year(text: str):\n",
    "    t = text.strip()\n",
    "\n",
    "    # \"1947\" / \"1978\"\n",
    "    if re.fullmatch(r\"\\d{4}\", t):\n",
    "        return Literal(t, datatype=XSD.gYear)\n",
    "\n",
    "    # \"6 December 1947\"\n",
    "    try:\n",
    "        dt = datetime.strptime(t, \"%d %B %Y\").date()\n",
    "        return Literal(dt.isoformat(), datatype=XSD.date)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdf74f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche URIs pour 23 entités...\n",
      "neural network -> http://dbpedia.org/resource/Neural_network\n",
      "Terry Sejnowski -> http://dbpedia.org/resource/Terry_Sejnowski\n",
      "Carnegie Mellon University -> http://dbpedia.org/resource/Carnegie_Mellon_University\n",
      "University of Toronto -> http://dbpedia.org/resource/Toronto\n",
      "David Ackley -> http://example.org/David_Ackley (Local)\n",
      "AlexNet -> http://example.org/AlexNet (Local)\n",
      "cognitive psychologist -> http://dbpedia.org/resource/Cognitive_psychology\n",
      "Alex Krizhevsky -> http://example.org/Alex_Krizhevsky (Local)\n",
      "Christopher Longuet-Higgins -> http://dbpedia.org/resource/Christopher_Longuet-Higgins\n",
      "DNNresearch Inc. -> http://example.org/DNNresearch_Inc. (Local)\n",
      "artificial neural network -> http://dbpedia.org/resource/Neural_network\n",
      "Google -> http://dbpedia.org/resource/Google\n",
      "David E. Rumelhart -> http://dbpedia.org/resource/David_Rumelhart\n",
      "machine learning -> http://dbpedia.org/resource/Machine_learning\n",
      "6 December 1947 -> http://example.org/6_December_1947 (Local)\n",
      "Everest Hinton -> http://dbpedia.org/resource/Mount_Everest\n",
      "Brendan Frey -> http://example.org/Brendan_Frey (Local)\n",
      "University of Edinburgh -> http://dbpedia.org/resource/Edinburgh\n",
      "computer scientist -> http://dbpedia.org/resource/Computer_scientist\n",
      "research papers -> http://example.org/research_papers (Local)\n",
      "Richard Zemel -> http://example.org/Richard_Zemel (Local)\n",
      "open-access -> http://dbpedia.org/resource/Open_access\n",
      "Boltzmann machine -> http://dbpedia.org/resource/Boltzmann_machine\n",
      "Ajout des triplets...\n",
      "\n",
      "========================================\n",
      " RÉSULTAT FINAL (.ttl) \n",
      "========================================\n",
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "<http://dbpedia.org/resource/Boltzmann_machine> ns1:discoverer_or_inventor <http://dbpedia.org/resource/Terry_Sejnowski>,\n",
      "        ns1:David_Ackley .\n",
      "\n",
      "<http://dbpedia.org/resource/Christopher_Longuet-Higgins> ns1:employer <http://dbpedia.org/resource/Edinburgh> .\n",
      "\n",
      "<http://dbpedia.org/resource/David_Rumelhart> ns1:educated_at <http://dbpedia.org/resource/Carnegie_Mellon_University> .\n",
      "\n",
      "<http://dbpedia.org/resource/Mount_Everest> ns1:date_of_birth ns1:6_December_1947 ;\n",
      "    ns1:employer <http://dbpedia.org/resource/Toronto> ;\n",
      "    ns1:field_of_work <http://dbpedia.org/resource/Neural_network> ;\n",
      "    ns1:occupation <http://dbpedia.org/resource/Computer_scientist> .\n",
      "\n",
      "<http://dbpedia.org/resource/Open_access> ns1:subclass_of ns1:research_papers .\n",
      "\n",
      "<http://dbpedia.org/resource/Google> ns1:subsidiary <http://example.org/DNNresearch_Inc.> .\n",
      "\n",
      "<http://dbpedia.org/resource/Neural_network> ns1:part_of <http://dbpedia.org/resource/Cognitive_psychology> ;\n",
      "    ns1:use <http://dbpedia.org/resource/Machine_learning> .\n",
      "\n",
      "ns1:AlexNet ns1:discoverer_or_inventor ns1:Alex_Krizhevsky .\n",
      "\n",
      "ns1:Alex_Krizhevsky ns1:notable_work ns1:AlexNet .\n",
      "\n",
      "ns1:Brendan_Frey ns1:student_of ns1:Richard_Zemel .\n",
      "\n",
      "<http://example.org/DNNresearch_Inc.> ns1:parent_organization <http://dbpedia.org/resource/Google> .\n",
      "\n",
      "ns1:Richard_Zemel ns1:student ns1:Brendan_Frey .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = Graph()\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "cache = {}\n",
    "\n",
    "all_names = set()\n",
    "for t in all_triplets:\n",
    "    all_names.add(t['head'])\n",
    "    all_names.add(t['tail'])\n",
    "\n",
    "print(f\"Recherche URIs pour {len(all_names)} entités...\")\n",
    "for nom in all_names:\n",
    "    uri = get_dbpedia_uri(nom)\n",
    "    if uri:\n",
    "        print(f\"{nom} -> {uri}\")\n",
    "        cache[nom] = URIRef(uri)\n",
    "    else:\n",
    "        # fallback\n",
    "        clean_nom = nom.replace(\" \", \"_\")\n",
    "        local_uri = EX[clean_nom]\n",
    "        print(f\"{nom} -> {local_uri} (Local)\")\n",
    "        cache[nom] = local_uri\n",
    "\n",
    "print(\"Ajout des triplets...\")\n",
    "for t in all_triplets:\n",
    "    s = cache[t['head']]\n",
    "    o = cache[t['tail']]\n",
    "    \n",
    "    # local relation\n",
    "    rel_clean = t['type'].replace(\" \", \"_\")\n",
    "    p = EX[rel_clean]\n",
    "    \n",
    "    g.add((s, p, o))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" RÉSULTAT FINAL (.ttl) \")\n",
    "print(\"=\"*40)\n",
    "print(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce71be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test diff values for conf, support\n",
    "def test(entities_list):\n",
    "    scenarios = [\n",
    "        (0.3, 0),    \n",
    "        (0.5, 20),  \n",
    "        (0.8, 100)  \n",
    "    ]\n",
    "    sample_entities = list(set(entities_list))[:4] \n",
    "    \n",
    "    for ent in sample_entities:\n",
    "        for conf, supp in scenarios:\n",
    "            uri = get_dbpedia_uri(ent, conf, supp)\n",
    "            res = uri.split('/')[-1] if uri else None\n",
    "            print(f\"{ent}: {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aabcc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open-access: http://dbpedia.org/resource/Open_access\n",
      "open-access: http://dbpedia.org/resource/Open_access\n",
      "open-access: None\n",
      "neural network: http://dbpedia.org/resource/Neural_network\n",
      "neural network: http://dbpedia.org/resource/Neural_network\n",
      "neural network: http://dbpedia.org/resource/Neural_network\n",
      "Terry Sejnowski: http://dbpedia.org/resource/Terry_Sejnowski\n",
      "Terry Sejnowski: http://dbpedia.org/resource/Terry_Sejnowski\n",
      "Terry Sejnowski: None\n",
      "Carnegie Mellon University: http://dbpedia.org/resource/Carnegie_Mellon_University\n",
      "Carnegie Mellon University: http://dbpedia.org/resource/Carnegie_Mellon_University\n",
      "Carnegie Mellon University: http://dbpedia.org/resource/Carnegie_Mellon_University\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N04dadb4fe85846228573e685afe06758 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build graph\n",
    "def build_final_graph(triplets):\n",
    "    g = Graph()\n",
    "    EX = Namespace(\"http://example.org/\") \n",
    "    \n",
    "    FINAL_CONF = 0.5\n",
    "    FINAL_SUPP = 20\n",
    "    \n",
    "    noms_uniques = set()\n",
    "    for t in triplets:\n",
    "        noms_uniques.add(t['head'])\n",
    "        noms_uniques.add(t['tail'])\n",
    "        \n",
    "    test(list(noms_uniques))\n",
    "        \n",
    "    uri_cache = {}\n",
    "    \n",
    "    for nom in noms_uniques:\n",
    "        uri = get_dbpedia_uri(nom, FINAL_CONF, FINAL_SUPP)\n",
    "        if uri:\n",
    "            uri_cache[nom] = URIRef(uri)\n",
    "        else:\n",
    "            clean = nom.replace(\" \", \"_\")\n",
    "            uri_cache[nom] = EX[clean]\n",
    "\n",
    "    for t in triplets:\n",
    "        s = uri_cache[t['head']]\n",
    "        o = uri_cache[t['tail']]\n",
    "        \n",
    "        rel_clean = t['type'].replace(\" \", \"_\").replace(\"-\", \"_\") # to get local relationships\n",
    "        p = EX[rel_clean]\n",
    "        \n",
    "        g.add((s, p, o))\n",
    "        \n",
    "    return g\n",
    "\n",
    "graph_final = build_final_graph(all_triplets)\n",
    "graph_final.serialize(\"graph.ttl\", format=\"turtle\")\n",
    "\n",
    "# print(graph_final.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d22e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Impact des paramètres \\texttt{confidence} et \\texttt{support} sur la couverture de DBpedia Spotlight (échantillon d’entités).}\n",
      "\\label{tab:spotlight_params}\n",
      "\\begin{tabular}{rrrrr}\n",
      "\\toprule\n",
      "confidence & support & entities_tested & linked_dbpedia & linked_pct \\\\\n",
      "\\midrule\n",
      "0.20 & 0 & 23 & 23 & 1.00 \\\\\n",
      "0.35 & 20 & 23 & 16 & 0.70 \\\\\n",
      "0.50 & 10 & 23 & 16 & 0.70 \\\\\n",
      "0.70 & 50 & 23 & 13 & 0.57 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'confidence': 0.2,\n",
       "  'support': 0,\n",
       "  'examples': [('6 December 1947', 'http://dbpedia.org/resource/December'),\n",
       "   ('Alex Krizhevsky', 'http://dbpedia.org/resource/Alex_Krizhevsky'),\n",
       "   ('AlexNet', 'http://dbpedia.org/resource/AlexNet')]},\n",
       " {'confidence': 0.35,\n",
       "  'support': 20,\n",
       "  'examples': [('6 December 1947', None),\n",
       "   ('Alex Krizhevsky', None),\n",
       "   ('AlexNet', None)]},\n",
       " {'confidence': 0.5,\n",
       "  'support': 10,\n",
       "  'examples': [('6 December 1947', None),\n",
       "   ('Alex Krizhevsky', None),\n",
       "   ('AlexNet', 'http://dbpedia.org/resource/AlexNet')]},\n",
       " {'confidence': 0.7,\n",
       "  'support': 50,\n",
       "  'examples': [('6 December 1947', None),\n",
       "   ('Alex Krizhevsky', None),\n",
       "   ('AlexNet', None)]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# 1) Entités uniques à tester (head+tail)\n",
    "entities = sorted({t[\"head\"] for t in all_triplets} | {t[\"tail\"] for t in all_triplets})\n",
    "\n",
    "# 2) Petite grille de tests (à ajuster si vous voulez)\n",
    "param_grid = [\n",
    "    (0.20, 0),\n",
    "    (0.35, 20),\n",
    "    (0.50, 10),\n",
    "    (0.70, 50),\n",
    "]\n",
    "\n",
    "# 3) Pour éviter de spammer l'API : on teste sur un petit échantillon stable\n",
    "N = min(25, len(entities))\n",
    "entities_sample = entities[:N]\n",
    "\n",
    "ENDPOINT = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "\n",
    "def spotlight_link(mention: str, confidence: float, support: int):\n",
    "    \"\"\"Renvoie une URI DBpedia (string) ou None.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            ENDPOINT,\n",
    "            params={\"text\": mention, \"confidence\": confidence, \"support\": support},\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "            timeout=15,\n",
    "        )\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        data = r.json()\n",
    "        resources = data.get(\"Resources\", [])\n",
    "        if not resources:\n",
    "            return None\n",
    "        # on prend la ressource la plus \"similaire\" si dispo\n",
    "        best = max(resources, key=lambda x: float(x.get(\"@similarityScore\", \"0\")))\n",
    "        return best.get(\"@URI\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "examples = []  # quelques exemples concrets (utile pour le texte du rapport)\n",
    "for conf, supp in param_grid:\n",
    "    linked = 0\n",
    "    local_examples = []\n",
    "    for e in entities_sample:\n",
    "        uri = spotlight_link(e, conf, supp)\n",
    "        time.sleep(0.2)  # évite les 403 si beaucoup d'appels\n",
    "        ok = uri is not None and \"dbpedia.org/resource/\" in uri\n",
    "        linked += int(ok)\n",
    "        if len(local_examples) < 3:  # garde 2-3 exemples\n",
    "            local_examples.append((e, uri))\n",
    "    rows.append({\n",
    "        \"confidence\": conf,\n",
    "        \"support\": supp,\n",
    "        \"entities_tested\": len(entities_sample),\n",
    "        \"linked_dbpedia\": linked,\n",
    "        \"linked_pct\": linked / len(entities_sample)\n",
    "    })\n",
    "    examples.append({\"confidence\": conf, \"support\": supp, \"examples\": local_examples})\n",
    "\n",
    "df_params = pd.DataFrame(rows).sort_values([\"linked_dbpedia\", \"confidence\"], ascending=[False, True])\n",
    "df_params\n",
    "\n",
    "# Export (optionnel) + génération LaTeX prête à coller dans le rapport\n",
    "df_params.to_csv(\"spotlight_grid_results.csv\", index=False)\n",
    "\n",
    "print(df_params.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.2f\",\n",
    "    caption=\"Impact des paramètres \\\\texttt{confidence} et \\\\texttt{support} sur la couverture de DBpedia Spotlight (échantillon d’entités).\",\n",
    "    label=\"tab:spotlight_params\"\n",
    "))\n",
    "\n",
    "# Bonus: affiche quelques exemples pour illustrer une erreur/ambiguïté\n",
    "examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
