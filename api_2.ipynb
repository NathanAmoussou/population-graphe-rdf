{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d950fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting rdflib\n",
      "  Using cached rdflib-7.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m829.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting pyparsing<4,>=2.1.0 (from rdflib)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "Using cached rdflib-7.5.0-py3-none-any.whl (587 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading urllib3-2.6.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, triton, tqdm, sympy, setuptools, safetensors, regex, pyyaml, pyparsing, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, rdflib, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.3 certifi-2025.11.12 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.12.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6 numpy-2.3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pyparsing-3.2.5 pyyaml-6.0.3 rdflib-7.5.0 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 tqdm-4.67.1 transformers-4.57.3 triton-3.5.1 typing-extensions-4.15.0 urllib3-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch rdflib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682b1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from rdflib import Graph, URIRef, Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2da50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte découpé avec succès en 9 blocs (paragraphes).\n",
      "Exemple du bloc 1 : Geoffrey Everest Hinton (born 6 December 1947) is an English Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013 he divides his time working for Google (Google Brain) and the University of Toronto. In 2017, he cofounded and became the Chief Scientific Advisor of the Vector Institute in Toronto.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'extraction REBEL...\n",
      "\n",
      "RAW OUTPUT BLOC 1 : '<s><triplet> Everest Hinton <subj> 6 December 1947 <obj> date of birth <subj> computer scientist <obj> occupation <subj> artificial neural network <obj> field of work <subj> University of Toronto <obj> employer <triplet> artificial neural network <subj> cognitive psychologist <obj> part of</s>'\n",
      "Bloc 1 : 5 triplets extraits.\n",
      "RAW OUTPUT BLOC 2 : '<s><triplet> AlexNet <subj> Alex Krizhevsky <obj> discoverer or inventor <triplet> Alex Krizhevsky <subj> AlexNet <obj> notable work</s>'\n",
      "Bloc 2 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 3 : '<s><triplet> Christopher Longuet-Higgins <subj> University of Edinburgh <obj> employer</s>'\n",
      "Bloc 3 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 4 : '<s><triplet> Google <subj> DNNresearch Inc. <obj> subsidiary <triplet> DNNresearch Inc. <subj> Google <obj> parent organization</s>'\n",
      "Bloc 4 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 5 : '<s><triplet> neural network <subj> machine learning <obj> use</s>'\n",
      "Bloc 5 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 6 : '<s><triplet> David E. Rumelhart <subj> Carnegie Mellon University <obj> educated at</s>'\n",
      "Bloc 6 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 7 : '<s><triplet> Boltzmann machine <subj> David Ackley <obj> discoverer or inventor <subj> Terry Sejnowski <obj> discoverer or inventor</s>'\n",
      "Bloc 7 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 8 : '<s><triplet> open-access <subj> research papers <obj> subclass of</s>'\n",
      "Bloc 8 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 9 : '<s><triplet> Richard Zemel <subj> Brendan Frey <obj> student <triplet> Brendan Frey <subj> Richard Zemel <obj> student of</s>'\n",
      "Bloc 9 : 2 triplets extraits.\n",
      "\n",
      "Terminé ! Total de triplets trouvés : 17\n",
      "[{'head': 'Everest Hinton', 'tail': '6 December 1947', 'type': 'date of birth'},\n",
      " {'head': 'Everest Hinton', 'tail': 'computer scientist', 'type': 'occupation'},\n",
      " {'head': 'Everest Hinton',\n",
      "  'tail': 'artificial neural network',\n",
      "  'type': 'field of work'},\n",
      " {'head': 'Everest Hinton',\n",
      "  'tail': 'University of Toronto',\n",
      "  'type': 'employer'},\n",
      " {'head': 'artificial neural network',\n",
      "  'tail': 'cognitive psychologist',\n",
      "  'type': 'part of'},\n",
      " {'head': 'AlexNet',\n",
      "  'tail': 'Alex Krizhevsky',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'Alex Krizhevsky', 'tail': 'AlexNet', 'type': 'notable work'},\n",
      " {'head': 'Christopher Longuet-Higgins',\n",
      "  'tail': 'University of Edinburgh',\n",
      "  'type': 'employer'},\n",
      " {'head': 'Google', 'tail': 'DNNresearch Inc.', 'type': 'subsidiary'},\n",
      " {'head': 'DNNresearch Inc.', 'tail': 'Google', 'type': 'parent organization'},\n",
      " {'head': 'neural network', 'tail': 'machine learning', 'type': 'use'},\n",
      " {'head': 'David E. Rumelhart',\n",
      "  'tail': 'Carnegie Mellon University',\n",
      "  'type': 'educated at'},\n",
      " {'head': 'Boltzmann machine',\n",
      "  'tail': 'David Ackley',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'Boltzmann machine',\n",
      "  'tail': 'Terry Sejnowski',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'open-access', 'tail': 'research papers', 'type': 'subclass of'},\n",
      " {'head': 'Richard Zemel', 'tail': 'Brendan Frey', 'type': 'student'},\n",
      " {'head': 'Brendan Frey', 'tail': 'Richard Zemel', 'type': 'student of'}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"texte_a_analyser.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_source = f.read()\n",
    "\n",
    "# Découpage : on sépare par les doubles sauts de ligne (\\n\\n) et on enlève les vides\n",
    "blocs = [b.strip() for b in text_source.split('\\n') if b.strip()]\n",
    "\n",
    "print(f\"Texte découpé avec succès en {len(blocs)} blocs (paragraphes).\")\n",
    "print(f\"Exemple du bloc 1 : {blocs[0]}\")\n",
    "\n",
    "# REBEL\n",
    "\n",
    "from transformers import pipeline\n",
    "import pprint\n",
    "\n",
    "# 1. Chargement du pipeline REBEL\n",
    "# Le premier lancement téléchargera le modèle (~1.6 Go)\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "# 2. Fonction minimale pour parser la sortie (String -> Liste de Dictionnaires)\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    # on enlève juste <s>, <pad>, </s> mais on garde <triplet>, <subj>, <obj>\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "\n",
    "# 3. Exécution sur les blocs\n",
    "print(\"Démarrage de l'extraction REBEL...\\n\")\n",
    "all_triplets = []\n",
    "\n",
    "for i, bloc in enumerate(blocs):\n",
    "    # 1) génération : on veut les ids, pas le texte nettoyé\n",
    "    out = triplet_extractor(\n",
    "        bloc,\n",
    "        max_length=512,\n",
    "        num_beams=3,\n",
    "        return_tensors=True,\n",
    "        return_text=False\n",
    "    )[0][\"generated_token_ids\"]\n",
    "\n",
    "    # 2) décodage manuel en gardant les tokens spéciaux\n",
    "    raw_output = triplet_extractor.tokenizer.batch_decode(\n",
    "        [out],\n",
    "        skip_special_tokens=False\n",
    "    )[0]\n",
    "\n",
    "    # Debug si tu veux voir la chaîne brute\n",
    "    print(f\"RAW OUTPUT BLOC {i+1} : {repr(raw_output)}\")\n",
    "\n",
    "    # 3) extraction des triplets\n",
    "    triplets = extract_triplets(raw_output)\n",
    "    all_triplets.extend(triplets)\n",
    "\n",
    "    print(f\"Bloc {i+1} : {len(triplets)} triplets extraits.\")\n",
    "\n",
    "print(f\"\\nTerminé ! Total de triplets trouvés : {len(all_triplets)}\")\n",
    "pprint.pprint(all_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3706ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_uri(text_entity, confidence=0.5, support=20):\n",
    "    api_url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "    params = {\n",
    "        \"text\": text_entity,\n",
    "        \"confidence\": confidence,\n",
    "        \"support\": support\n",
    "    }\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(0.2) \n",
    "        response = requests.get(api_url, params=params, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'Resources' in data:\n",
    "                return data['Resources'][0]['@URI']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a72e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test diff values for conf, support\n",
    "def test(entities_list):\n",
    "    scenarios = [\n",
    "        (0.3, 0),    \n",
    "        (0.5, 20),  \n",
    "        (0.8, 100)  \n",
    "    ]\n",
    "    sample_entities = list(set(entities_list))[:4] \n",
    "    \n",
    "    for ent in sample_entities:\n",
    "        for conf, supp in scenarios:\n",
    "            uri = get_dbpedia_uri(ent, conf, supp)\n",
    "            res = uri.split('/')[-1] if uri else None\n",
    "            print(f\"{ent}: {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96be1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "def build_final_graph(triplets):\n",
    "    g = Graph()\n",
    "    EX = Namespace(\"http://example.org/\") \n",
    "    \n",
    "    FINAL_CONF = 0.5\n",
    "    FINAL_SUPP = 20\n",
    "    \n",
    "    noms_uniques = set()\n",
    "    for t in triplets:\n",
    "        noms_uniques.add(t['head'])\n",
    "        noms_uniques.add(t['tail'])\n",
    "        \n",
    "    test(list(noms_uniques))\n",
    "        \n",
    "    uri_cache = {}\n",
    "    \n",
    "    for nom in noms_uniques:\n",
    "        uri = get_dbpedia_uri(nom, FINAL_CONF, FINAL_SUPP)\n",
    "        if uri:\n",
    "            uri_cache[nom] = URIRef(uri)\n",
    "        else:\n",
    "            clean = nom.replace(\" \", \"_\")\n",
    "            uri_cache[nom] = EX[clean]\n",
    "\n",
    "    for t in triplets:\n",
    "        s = uri_cache[t['head']]\n",
    "        o = uri_cache[t['tail']]\n",
    "        \n",
    "        rel_clean = t['type'].replace(\" \", \"_\").replace(\"-\", \"_\") # to get local relationships\n",
    "        p = EX[rel_clean]\n",
    "        \n",
    "        g.add((s, p, o))\n",
    "        \n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e4feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to try\n",
    "triplets_input = [\n",
    "    {'head': 'Geoffrey Hinton', 'type': 'works at', 'tail': 'Google'},\n",
    "    {'head': 'Geoffrey Hinton', 'type': 'born in', 'tail': '1947'},\n",
    "    {'head': 'AlexNet', 'type': 'designed by', 'tail': 'Alex Krizhevsky'},\n",
    "    {'head': 'University of Toronto', 'type': 'employer', 'tail': 'Geoffrey Hinton'},\n",
    "    {'head': 'Vector Institute', 'type': 'located in', 'tail': 'Toronto'},\n",
    "    {'head': 'Deep Learning', 'type': 'field of work', 'tail': 'Yann LeCun'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ffc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = list(set([t['head'] for t in all_triplets] + [t['tail'] for t in all_triplets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645ee29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University of Toronto: http://dbpedia.org/resource/University\n",
      "University of Toronto: http://dbpedia.org/resource/Toronto\n",
      "University of Toronto: http://dbpedia.org/resource/Toronto\n",
      "Vector Institute: http://dbpedia.org/resource/State_Research_Center_of_Virology_and_Biotechnology_VECTOR\n",
      "Vector Institute: http://dbpedia.org/resource/State_Research_Center_of_Virology_and_Biotechnology_VECTOR\n",
      "Vector Institute: None\n",
      "Geoffrey Hinton: http://dbpedia.org/resource/Geoffrey_Hinton\n",
      "Geoffrey Hinton: http://dbpedia.org/resource/Geoffrey_Hinton\n",
      "Geoffrey Hinton: http://dbpedia.org/resource/Geoffrey_Hinton\n",
      "Toronto: http://dbpedia.org/resource/Toronto\n",
      "Toronto: http://dbpedia.org/resource/Toronto\n",
      "Toronto: http://dbpedia.org/resource/Toronto\n"
     ]
    }
   ],
   "source": [
    "graph_final = build_final_graph(triplets_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2f124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "<http://dbpedia.org/resource/Deep_learning> ns1:field_of_work <http://dbpedia.org/resource/Yann_LeCun> .\n",
      "\n",
      "<http://dbpedia.org/resource/State_Research_Center_of_Virology_and_Biotechnology_VECTOR> ns1:located_in <http://dbpedia.org/resource/Toronto> .\n",
      "\n",
      "ns1:AlexNet ns1:designed_by ns1:Alex_Krizhevsky .\n",
      "\n",
      "<http://dbpedia.org/resource/Geoffrey_Hinton> ns1:born_in ns1:1947 ;\n",
      "    ns1:works_at <http://dbpedia.org/resource/Google> .\n",
      "\n",
      "<http://dbpedia.org/resource/Toronto> ns1:employer <http://dbpedia.org/resource/Geoffrey_Hinton> .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph_final.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
