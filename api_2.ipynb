{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d950fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: rdflib in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (7.5.0)\n",
      "Requirement already satisfied: requests in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from rdflib) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch rdflib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682b1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from rdflib import Graph, URIRef, Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2da50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte dÃ©coupÃ© avec succÃ¨s en 9 blocs (paragraphes).\n",
      "Exemple du bloc 1 : Geoffrey Everest Hinton (born 6 December 1947) is an English Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013 he divides his time working for Google (Google Brain) and the University of Toronto. In 2017, he cofounded and became the Chief Scientific Advisor of the Vector Institute in Toronto.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DÃ©marrage de l'extraction REBEL...\n",
      "\n",
      "RAW OUTPUT BLOC 1 : '<s><triplet> Everest Hinton <subj> 6 December 1947 <obj> date of birth <subj> computer scientist <obj> occupation <subj> artificial neural network <obj> field of work <subj> University of Toronto <obj> employer <triplet> artificial neural network <subj> cognitive psychologist <obj> part of</s>'\n",
      "Bloc 1 : 5 triplets extraits.\n",
      "RAW OUTPUT BLOC 2 : '<s><triplet> AlexNet <subj> Alex Krizhevsky <obj> discoverer or inventor <triplet> Alex Krizhevsky <subj> AlexNet <obj> notable work</s>'\n",
      "Bloc 2 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 3 : '<s><triplet> Christopher Longuet-Higgins <subj> University of Edinburgh <obj> employer</s>'\n",
      "Bloc 3 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 4 : '<s><triplet> Google <subj> DNNresearch Inc. <obj> subsidiary <triplet> DNNresearch Inc. <subj> Google <obj> parent organization</s>'\n",
      "Bloc 4 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 5 : '<s><triplet> neural network <subj> machine learning <obj> use</s>'\n",
      "Bloc 5 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 6 : '<s><triplet> David E. Rumelhart <subj> Carnegie Mellon University <obj> educated at</s>'\n",
      "Bloc 6 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 7 : '<s><triplet> Boltzmann machine <subj> David Ackley <obj> discoverer or inventor <subj> Terry Sejnowski <obj> discoverer or inventor</s>'\n",
      "Bloc 7 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 8 : '<s><triplet> open-access <subj> research papers <obj> subclass of</s>'\n",
      "Bloc 8 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 9 : '<s><triplet> Richard Zemel <subj> Brendan Frey <obj> student <triplet> Brendan Frey <subj> Richard Zemel <obj> student of</s>'\n",
      "Bloc 9 : 2 triplets extraits.\n",
      "\n",
      "TerminÃ© ! Total de triplets trouvÃ©s : 17\n",
      "[{'head': 'Everest Hinton', 'tail': '6 December 1947', 'type': 'date of birth'},\n",
      " {'head': 'Everest Hinton', 'tail': 'computer scientist', 'type': 'occupation'},\n",
      " {'head': 'Everest Hinton',\n",
      "  'tail': 'artificial neural network',\n",
      "  'type': 'field of work'},\n",
      " {'head': 'Everest Hinton',\n",
      "  'tail': 'University of Toronto',\n",
      "  'type': 'employer'},\n",
      " {'head': 'artificial neural network',\n",
      "  'tail': 'cognitive psychologist',\n",
      "  'type': 'part of'},\n",
      " {'head': 'AlexNet',\n",
      "  'tail': 'Alex Krizhevsky',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'Alex Krizhevsky', 'tail': 'AlexNet', 'type': 'notable work'},\n",
      " {'head': 'Christopher Longuet-Higgins',\n",
      "  'tail': 'University of Edinburgh',\n",
      "  'type': 'employer'},\n",
      " {'head': 'Google', 'tail': 'DNNresearch Inc.', 'type': 'subsidiary'},\n",
      " {'head': 'DNNresearch Inc.', 'tail': 'Google', 'type': 'parent organization'},\n",
      " {'head': 'neural network', 'tail': 'machine learning', 'type': 'use'},\n",
      " {'head': 'David E. Rumelhart',\n",
      "  'tail': 'Carnegie Mellon University',\n",
      "  'type': 'educated at'},\n",
      " {'head': 'Boltzmann machine',\n",
      "  'tail': 'David Ackley',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'Boltzmann machine',\n",
      "  'tail': 'Terry Sejnowski',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'open-access', 'tail': 'research papers', 'type': 'subclass of'},\n",
      " {'head': 'Richard Zemel', 'tail': 'Brendan Frey', 'type': 'student'},\n",
      " {'head': 'Brendan Frey', 'tail': 'Richard Zemel', 'type': 'student of'}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"texte_a_analyser.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_source = f.read()\n",
    "\n",
    "# DÃ©coupage : on sÃ©pare par les doubles sauts de ligne (\\n\\n) et on enlÃ¨ve les vides\n",
    "blocs = [b.strip() for b in text_source.split('\\n') if b.strip()]\n",
    "\n",
    "print(f\"Texte dÃ©coupÃ© avec succÃ¨s en {len(blocs)} blocs (paragraphes).\")\n",
    "print(f\"Exemple du bloc 1 : {blocs[0]}\")\n",
    "\n",
    "# REBEL\n",
    "\n",
    "from transformers import pipeline\n",
    "import pprint\n",
    "\n",
    "# 1. Chargement du pipeline REBEL\n",
    "# Le premier lancement tÃ©lÃ©chargera le modÃ¨le (~1.6 Go)\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "# 2. Fonction minimale pour parser la sortie (String -> Liste de Dictionnaires)\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    # on enlÃ¨ve juste <s>, <pad>, </s> mais on garde <triplet>, <subj>, <obj>\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "\n",
    "# 3. ExÃ©cution sur les blocs\n",
    "print(\"DÃ©marrage de l'extraction REBEL...\\n\")\n",
    "all_triplets = []\n",
    "\n",
    "for i, bloc in enumerate(blocs):\n",
    "    # 1) gÃ©nÃ©ration : on veut les ids, pas le texte nettoyÃ©\n",
    "    out = triplet_extractor(\n",
    "        bloc,\n",
    "        max_length=512,\n",
    "        num_beams=3,\n",
    "        return_tensors=True,\n",
    "        return_text=False\n",
    "    )[0][\"generated_token_ids\"]\n",
    "\n",
    "    # 2) dÃ©codage manuel en gardant les tokens spÃ©ciaux\n",
    "    raw_output = triplet_extractor.tokenizer.batch_decode(\n",
    "        [out],\n",
    "        skip_special_tokens=False\n",
    "    )[0]\n",
    "\n",
    "    # Debug si tu veux voir la chaÃ®ne brute\n",
    "    print(f\"RAW OUTPUT BLOC {i+1} : {repr(raw_output)}\")\n",
    "\n",
    "    # 3) extraction des triplets\n",
    "    triplets = extract_triplets(raw_output)\n",
    "    all_triplets.extend(triplets)\n",
    "\n",
    "    print(f\"Bloc {i+1} : {len(triplets)} triplets extraits.\")\n",
    "\n",
    "print(f\"\\nTerminÃ© ! Total de triplets trouvÃ©s : {len(all_triplets)}\")\n",
    "pprint.pprint(all_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3706ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_uri(text_entity, confidence=0.5, support=20):\n",
    "    api_url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "    params = {\n",
    "        \"text\": text_entity,\n",
    "        \"confidence\": confidence,\n",
    "        \"support\": support\n",
    "    }\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(0.2) \n",
    "        response = requests.get(api_url, params=params, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'Resources' in data:\n",
    "                return data['Resources'][0]['@URI']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06e73142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Recherche URIs pour 23 entitÃ©s...\n",
      "   âœ… University of Toronto -> http://dbpedia.org/resource/Toronto\n",
      "   âœ… cognitive psychologist -> http://dbpedia.org/resource/Cognitive_psychology\n",
      "   âœ… machine learning -> http://dbpedia.org/resource/Machine_learning\n",
      "   âœ… neural network -> http://dbpedia.org/resource/Neural_network\n",
      "   âœ… Google -> http://dbpedia.org/resource/Google\n",
      "   âš ï¸ Alex Krizhevsky -> http://example.org/Alex_Krizhevsky (Local)\n",
      "   âœ… University of Edinburgh -> http://dbpedia.org/resource/Edinburgh\n",
      "   âœ… Everest Hinton -> http://dbpedia.org/resource/Mount_Everest\n",
      "   âš ï¸ DNNresearch Inc. -> http://example.org/DNNresearch_Inc. (Local)\n",
      "   âœ… Terry Sejnowski -> http://dbpedia.org/resource/Terry_Sejnowski\n",
      "   âœ… computer scientist -> http://dbpedia.org/resource/Computer_scientist\n",
      "   âœ… Christopher Longuet-Higgins -> http://dbpedia.org/resource/Christopher_Longuet-Higgins\n",
      "   âœ… David E. Rumelhart -> http://dbpedia.org/resource/David_Rumelhart\n",
      "   âœ… artificial neural network -> http://dbpedia.org/resource/Neural_network\n",
      "   âœ… Carnegie Mellon University -> http://dbpedia.org/resource/Carnegie_Mellon_University\n",
      "   âš ï¸ Richard Zemel -> http://example.org/Richard_Zemel (Local)\n",
      "   âš ï¸ 6 December 1947 -> http://example.org/6_December_1947 (Local)\n",
      "   âš ï¸ research papers -> http://example.org/research_papers (Local)\n",
      "   âš ï¸ AlexNet -> http://example.org/AlexNet (Local)\n",
      "   âœ… open-access -> http://dbpedia.org/resource/Open_access\n",
      "   âš ï¸ Brendan Frey -> http://example.org/Brendan_Frey (Local)\n",
      "   âœ… Boltzmann machine -> http://dbpedia.org/resource/Boltzmann_machine\n",
      "   âš ï¸ David Ackley -> http://example.org/David_Ackley (Local)\n",
      "ðŸ”— Ajout des triplets...\n",
      "\n",
      "========================================\n",
      " RÃ‰SULTAT FINAL (.ttl) \n",
      "========================================\n",
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "<http://dbpedia.org/resource/Boltzmann_machine> ns1:discoverer_or_inventor <http://dbpedia.org/resource/Terry_Sejnowski>,\n",
      "        ns1:David_Ackley .\n",
      "\n",
      "<http://dbpedia.org/resource/Christopher_Longuet-Higgins> ns1:employer <http://dbpedia.org/resource/Edinburgh> .\n",
      "\n",
      "<http://dbpedia.org/resource/David_Rumelhart> ns1:educated_at <http://dbpedia.org/resource/Carnegie_Mellon_University> .\n",
      "\n",
      "<http://dbpedia.org/resource/Mount_Everest> ns1:date_of_birth ns1:6_December_1947 ;\n",
      "    ns1:employer <http://dbpedia.org/resource/Toronto> ;\n",
      "    ns1:field_of_work <http://dbpedia.org/resource/Neural_network> ;\n",
      "    ns1:occupation <http://dbpedia.org/resource/Computer_scientist> .\n",
      "\n",
      "<http://dbpedia.org/resource/Open_access> ns1:subclass_of ns1:research_papers .\n",
      "\n",
      "<http://dbpedia.org/resource/Google> ns1:subsidiary <http://example.org/DNNresearch_Inc.> .\n",
      "\n",
      "<http://dbpedia.org/resource/Neural_network> ns1:part_of <http://dbpedia.org/resource/Cognitive_psychology> ;\n",
      "    ns1:use <http://dbpedia.org/resource/Machine_learning> .\n",
      "\n",
      "ns1:AlexNet ns1:discoverer_or_inventor ns1:Alex_Krizhevsky .\n",
      "\n",
      "ns1:Alex_Krizhevsky ns1:notable_work ns1:AlexNet .\n",
      "\n",
      "ns1:Brendan_Frey ns1:student_of ns1:Richard_Zemel .\n",
      "\n",
      "<http://example.org/DNNresearch_Inc.> ns1:parent_organization <http://dbpedia.org/resource/Google> .\n",
      "\n",
      "ns1:Richard_Zemel ns1:student ns1:Brendan_Frey .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = Graph()\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "cache = {}\n",
    "\n",
    "# 1. Lister tous les mots uniques\n",
    "all_names = set()\n",
    "for t in all_triplets:\n",
    "    all_names.add(t['head'])\n",
    "    all_names.add(t['tail'])\n",
    "\n",
    "# 2. Chercher les URIs\n",
    "print(f\"ðŸ”Ž Recherche URIs pour {len(all_names)} entitÃ©s...\")\n",
    "for nom in all_names:\n",
    "    uri = get_dbpedia_uri(nom)\n",
    "    if uri:\n",
    "        print(f\"   âœ… {nom} -> {uri}\")\n",
    "        cache[nom] = URIRef(uri)\n",
    "    else:\n",
    "        # Fallback\n",
    "        clean_nom = nom.replace(\" \", \"_\")\n",
    "        local_uri = EX[clean_nom]\n",
    "        print(f\"   âš ï¸ {nom} -> {local_uri} (Local)\")\n",
    "        cache[nom] = local_uri\n",
    "\n",
    "# 3. Ajouter les triplets au graphe\n",
    "print(\"ðŸ”— Ajout des triplets...\")\n",
    "for t in all_triplets:\n",
    "    s = cache[t['head']]\n",
    "    o = cache[t['tail']]\n",
    "    \n",
    "    # Relation locale\n",
    "    rel_clean = t['type'].replace(\" \", \"_\")\n",
    "    p = EX[rel_clean]\n",
    "    \n",
    "    g.add((s, p, o))\n",
    "\n",
    "# 4. AFFICHER LE RESULTAT\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" RÃ‰SULTAT FINAL (.ttl) \")\n",
    "print(\"=\"*40)\n",
    "print(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a72e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test diff values for conf, support\n",
    "def test(entities_list):\n",
    "    scenarios = [\n",
    "        (0.3, 0),    \n",
    "        (0.5, 20),  \n",
    "        (0.8, 100)  \n",
    "    ]\n",
    "    sample_entities = list(set(entities_list))[:4] \n",
    "    \n",
    "    for ent in sample_entities:\n",
    "        for conf, supp in scenarios:\n",
    "            uri = get_dbpedia_uri(ent, conf, supp)\n",
    "            res = uri.split('/')[-1] if uri else None\n",
    "            print(f\"{ent}: {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96be1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "def build_final_graph(triplets):\n",
    "    g = Graph()\n",
    "    EX = Namespace(\"http://example.org/\") \n",
    "    \n",
    "    FINAL_CONF = 0.5\n",
    "    FINAL_SUPP = 20\n",
    "    \n",
    "    noms_uniques = set()\n",
    "    for t in triplets:\n",
    "        noms_uniques.add(t['head'])\n",
    "        noms_uniques.add(t['tail'])\n",
    "        \n",
    "    test(list(noms_uniques))\n",
    "        \n",
    "    uri_cache = {}\n",
    "    \n",
    "    for nom in noms_uniques:\n",
    "        uri = get_dbpedia_uri(nom, FINAL_CONF, FINAL_SUPP)\n",
    "        if uri:\n",
    "            uri_cache[nom] = URIRef(uri)\n",
    "        else:\n",
    "            clean = nom.replace(\" \", \"_\")\n",
    "            uri_cache[nom] = EX[clean]\n",
    "\n",
    "    for t in triplets:\n",
    "        s = uri_cache[t['head']]\n",
    "        o = uri_cache[t['tail']]\n",
    "        \n",
    "        rel_clean = t['type'].replace(\" \", \"_\").replace(\"-\", \"_\") # to get local relationships\n",
    "        p = EX[rel_clean]\n",
    "        \n",
    "        g.add((s, p, o))\n",
    "        \n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to try\n",
    "all_triplets = [\n",
    "    {'head': 'Geoffrey Hinton', 'type': 'works at', 'tail': 'Google'},\n",
    "    {'head': 'Geoffrey Hinton', 'type': 'born in', 'tail': '1947'},\n",
    "    {'head': 'AlexNet', 'type': 'designed by', 'tail': 'Alex Krizhevsky'},\n",
    "    {'head': 'University of Toronto', 'type': 'employer', 'tail': 'Geoffrey Hinton'},\n",
    "    {'head': 'Vector Institute', 'type': 'located in', 'tail': 'Toronto'},\n",
    "    {'head': 'Deep Learning', 'type': 'field of work', 'tail': 'Yann LeCun'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e64ffc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = list(set([t['head'] for t in all_triplets] + [t['tail'] for t in all_triplets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "645ee29a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m graph_final = \u001b[43mbuild_final_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mbuild_final_graph\u001b[39m\u001b[34m(triplets)\u001b[39m\n\u001b[32m      9\u001b[39m noms_uniques = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m triplets:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     noms_uniques.add(\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhead\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     12\u001b[39m     noms_uniques.add(t[\u001b[33m'\u001b[39m\u001b[33mtail\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     14\u001b[39m test(\u001b[38;5;28mlist\u001b[39m(noms_uniques))\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "graph_final = build_final_graph(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2f124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "<http://dbpedia.org/resource/Deep_learning> ns1:field_of_work <http://dbpedia.org/resource/Yann_LeCun> .\n",
      "\n",
      "<http://dbpedia.org/resource/State_Research_Center_of_Virology_and_Biotechnology_VECTOR> ns1:located_in <http://dbpedia.org/resource/Toronto> .\n",
      "\n",
      "ns1:AlexNet ns1:designed_by ns1:Alex_Krizhevsky .\n",
      "\n",
      "<http://dbpedia.org/resource/Geoffrey_Hinton> ns1:born_in ns1:1947 ;\n",
      "    ns1:works_at <http://dbpedia.org/resource/Google> .\n",
      "\n",
      "<http://dbpedia.org/resource/Toronto> ns1:employer <http://dbpedia.org/resource/Geoffrey_Hinton> .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph_final.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
