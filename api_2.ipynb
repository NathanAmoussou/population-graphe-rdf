{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d950fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: rdflib in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (7.5.0)\n",
      "Requirement already satisfied: requests in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from rdflib) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch rdflib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682b1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from rdflib import Graph, URIRef, Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2da50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte découpé avec succès en 9 blocs (paragraphes).\n",
      "Exemple du bloc 1 : Geoffrey Everest Hinton (born 6 December 1947) is an English Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013 he divides his time working for Google (Google Brain) and the University of Toronto. In 2017, he cofounded and became the Chief Scientific Advisor of the Vector Institute in Toronto.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurine/Documents/M2 IA/KNOWLEDGE ENG./TP3/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'extraction REBEL...\n",
      "\n",
      "RAW OUTPUT BLOC 1 : '<s><triplet> Everest Hinton <subj> 6 December 1947 <obj> date of birth <subj> computer scientist <obj> occupation <subj> artificial neural network <obj> field of work <subj> University of Toronto <obj> employer <triplet> artificial neural network <subj> cognitive psychologist <obj> part of</s>'\n",
      "Bloc 1 : 5 triplets extraits.\n",
      "RAW OUTPUT BLOC 2 : '<s><triplet> AlexNet <subj> Alex Krizhevsky <obj> discoverer or inventor <triplet> Alex Krizhevsky <subj> AlexNet <obj> notable work</s>'\n",
      "Bloc 2 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 3 : '<s><triplet> Christopher Longuet-Higgins <subj> University of Edinburgh <obj> employer</s>'\n",
      "Bloc 3 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 4 : '<s><triplet> Google <subj> DNNresearch Inc. <obj> subsidiary <triplet> DNNresearch Inc. <subj> Google <obj> parent organization</s>'\n",
      "Bloc 4 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 5 : '<s><triplet> neural network <subj> machine learning <obj> use</s>'\n",
      "Bloc 5 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 6 : '<s><triplet> David E. Rumelhart <subj> Carnegie Mellon University <obj> educated at</s>'\n",
      "Bloc 6 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 7 : '<s><triplet> Boltzmann machine <subj> David Ackley <obj> discoverer or inventor <subj> Terry Sejnowski <obj> discoverer or inventor</s>'\n",
      "Bloc 7 : 2 triplets extraits.\n",
      "RAW OUTPUT BLOC 8 : '<s><triplet> open-access <subj> research papers <obj> subclass of</s>'\n",
      "Bloc 8 : 1 triplets extraits.\n",
      "RAW OUTPUT BLOC 9 : '<s><triplet> Richard Zemel <subj> Brendan Frey <obj> student <triplet> Brendan Frey <subj> Richard Zemel <obj> student of</s>'\n",
      "Bloc 9 : 2 triplets extraits.\n",
      "\n",
      "Terminé ! Total de triplets trouvés : 17\n",
      "[{'head': 'Everest Hinton', 'tail': '6 December 1947', 'type': 'date of birth'},\n",
      " {'head': 'Everest Hinton', 'tail': 'computer scientist', 'type': 'occupation'},\n",
      " {'head': 'Everest Hinton',\n",
      "  'tail': 'artificial neural network',\n",
      "  'type': 'field of work'},\n",
      " {'head': 'Everest Hinton',\n",
      "  'tail': 'University of Toronto',\n",
      "  'type': 'employer'},\n",
      " {'head': 'artificial neural network',\n",
      "  'tail': 'cognitive psychologist',\n",
      "  'type': 'part of'},\n",
      " {'head': 'AlexNet',\n",
      "  'tail': 'Alex Krizhevsky',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'Alex Krizhevsky', 'tail': 'AlexNet', 'type': 'notable work'},\n",
      " {'head': 'Christopher Longuet-Higgins',\n",
      "  'tail': 'University of Edinburgh',\n",
      "  'type': 'employer'},\n",
      " {'head': 'Google', 'tail': 'DNNresearch Inc.', 'type': 'subsidiary'},\n",
      " {'head': 'DNNresearch Inc.', 'tail': 'Google', 'type': 'parent organization'},\n",
      " {'head': 'neural network', 'tail': 'machine learning', 'type': 'use'},\n",
      " {'head': 'David E. Rumelhart',\n",
      "  'tail': 'Carnegie Mellon University',\n",
      "  'type': 'educated at'},\n",
      " {'head': 'Boltzmann machine',\n",
      "  'tail': 'David Ackley',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'Boltzmann machine',\n",
      "  'tail': 'Terry Sejnowski',\n",
      "  'type': 'discoverer or inventor'},\n",
      " {'head': 'open-access', 'tail': 'research papers', 'type': 'subclass of'},\n",
      " {'head': 'Richard Zemel', 'tail': 'Brendan Frey', 'type': 'student'},\n",
      " {'head': 'Brendan Frey', 'tail': 'Richard Zemel', 'type': 'student of'}]\n"
     ]
    }
   ],
   "source": [
    "with open(\"texte_a_analyser.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_source = f.read()\n",
    "\n",
    "# Découpage : on sépare par les doubles sauts de ligne (\\n\\n) et on enlève les vides\n",
    "blocs = [b.strip() for b in text_source.split('\\n') if b.strip()]\n",
    "\n",
    "print(f\"Texte découpé avec succès en {len(blocs)} blocs (paragraphes).\")\n",
    "print(f\"Exemple du bloc 1 : {blocs[0]}\")\n",
    "\n",
    "# REBEL\n",
    "\n",
    "from transformers import pipeline\n",
    "import pprint\n",
    "\n",
    "# 1. Chargement du pipeline REBEL\n",
    "# Le premier lancement téléchargera le modèle (~1.6 Go)\n",
    "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
    "\n",
    "# 2. Fonction minimale pour parser la sortie (String -> Liste de Dictionnaires)\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, object_ = '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    # on enlève juste <s>, <pad>, </s> mais on garde <triplet>, <subj>, <obj>\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(), 'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "\n",
    "# 3. Exécution sur les blocs\n",
    "print(\"Démarrage de l'extraction REBEL...\\n\")\n",
    "all_triplets = []\n",
    "\n",
    "for i, bloc in enumerate(blocs):\n",
    "    # 1) génération : on veut les ids, pas le texte nettoyé\n",
    "    out = triplet_extractor(\n",
    "        bloc,\n",
    "        max_length=512,\n",
    "        num_beams=3,\n",
    "        return_tensors=True,\n",
    "        return_text=False\n",
    "    )[0][\"generated_token_ids\"]\n",
    "\n",
    "    # 2) décodage manuel en gardant les tokens spéciaux\n",
    "    raw_output = triplet_extractor.tokenizer.batch_decode(\n",
    "        [out],\n",
    "        skip_special_tokens=False\n",
    "    )[0]\n",
    "\n",
    "    # Debug si tu veux voir la chaîne brute\n",
    "    print(f\"RAW OUTPUT BLOC {i+1} : {repr(raw_output)}\")\n",
    "\n",
    "    # 3) extraction des triplets\n",
    "    triplets = extract_triplets(raw_output)\n",
    "    all_triplets.extend(triplets)\n",
    "\n",
    "    print(f\"Bloc {i+1} : {len(triplets)} triplets extraits.\")\n",
    "\n",
    "print(f\"\\nTerminé ! Total de triplets trouvés : {len(all_triplets)}\")\n",
    "pprint.pprint(all_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3706ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_uri(text_entity, confidence=0.5, support=20):\n",
    "    api_url = \"https://api.dbpedia-spotlight.org/en/annotate\"\n",
    "    params = {\n",
    "        \"text\": text_entity,\n",
    "        \"confidence\": confidence,\n",
    "        \"support\": support\n",
    "    }\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        time.sleep(0.2) \n",
    "        response = requests.get(api_url, params=params, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if 'Resources' in data:\n",
    "                return data['Resources'][0]['@URI']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e73142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche URIs pour 23 entités...\n",
      "Boltzmann machine -> http://dbpedia.org/resource/Boltzmann_machine\n",
      "artificial neural network -> http://dbpedia.org/resource/Neural_network\n",
      "Terry Sejnowski -> http://dbpedia.org/resource/Terry_Sejnowski\n",
      "cognitive psychologist -> http://dbpedia.org/resource/Cognitive_psychology\n",
      "machine learning -> http://dbpedia.org/resource/Machine_learning\n",
      "David E. Rumelhart -> http://dbpedia.org/resource/David_Rumelhart\n",
      "Google -> http://dbpedia.org/resource/Google\n",
      "David Ackley -> http://example.org/David_Ackley (Local)\n",
      "Christopher Longuet-Higgins -> http://dbpedia.org/resource/Christopher_Longuet-Higgins\n",
      "6 December 1947 -> http://example.org/6_December_1947 (Local)\n",
      "Alex Krizhevsky -> http://example.org/Alex_Krizhevsky (Local)\n",
      "neural network -> http://dbpedia.org/resource/Neural_network\n",
      "research papers -> http://example.org/research_papers (Local)\n",
      "open-access -> http://dbpedia.org/resource/Open_access\n",
      "Carnegie Mellon University -> http://dbpedia.org/resource/Carnegie_Mellon_University\n",
      "Brendan Frey -> http://example.org/Brendan_Frey (Local)\n",
      "Everest Hinton -> http://dbpedia.org/resource/Mount_Everest\n",
      "computer scientist -> http://dbpedia.org/resource/Computer_scientist\n",
      "University of Toronto -> http://dbpedia.org/resource/Toronto\n",
      "DNNresearch Inc. -> http://example.org/DNNresearch_Inc. (Local)\n",
      "AlexNet -> http://example.org/AlexNet (Local)\n",
      "Richard Zemel -> http://example.org/Richard_Zemel (Local)\n",
      "University of Edinburgh -> http://dbpedia.org/resource/Edinburgh\n",
      "Ajout des triplets...\n",
      "\n",
      "========================================\n",
      " RÉSULTAT FINAL (.ttl) \n",
      "========================================\n",
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "<http://dbpedia.org/resource/Boltzmann_machine> ns1:discoverer_or_inventor <http://dbpedia.org/resource/Terry_Sejnowski>,\n",
      "        ns1:David_Ackley .\n",
      "\n",
      "<http://dbpedia.org/resource/Christopher_Longuet-Higgins> ns1:employer <http://dbpedia.org/resource/Edinburgh> .\n",
      "\n",
      "<http://dbpedia.org/resource/David_Rumelhart> ns1:educated_at <http://dbpedia.org/resource/Carnegie_Mellon_University> .\n",
      "\n",
      "<http://dbpedia.org/resource/Mount_Everest> ns1:date_of_birth ns1:6_December_1947 ;\n",
      "    ns1:employer <http://dbpedia.org/resource/Toronto> ;\n",
      "    ns1:field_of_work <http://dbpedia.org/resource/Neural_network> ;\n",
      "    ns1:occupation <http://dbpedia.org/resource/Computer_scientist> .\n",
      "\n",
      "<http://dbpedia.org/resource/Open_access> ns1:subclass_of ns1:research_papers .\n",
      "\n",
      "<http://dbpedia.org/resource/Google> ns1:subsidiary <http://example.org/DNNresearch_Inc.> .\n",
      "\n",
      "<http://dbpedia.org/resource/Neural_network> ns1:part_of <http://dbpedia.org/resource/Cognitive_psychology> ;\n",
      "    ns1:use <http://dbpedia.org/resource/Machine_learning> .\n",
      "\n",
      "ns1:AlexNet ns1:discoverer_or_inventor ns1:Alex_Krizhevsky .\n",
      "\n",
      "ns1:Alex_Krizhevsky ns1:notable_work ns1:AlexNet .\n",
      "\n",
      "ns1:Brendan_Frey ns1:student_of ns1:Richard_Zemel .\n",
      "\n",
      "<http://example.org/DNNresearch_Inc.> ns1:parent_organization <http://dbpedia.org/resource/Google> .\n",
      "\n",
      "ns1:Richard_Zemel ns1:student ns1:Brendan_Frey .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = Graph()\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "cache = {}\n",
    "\n",
    "all_names = set()\n",
    "for t in all_triplets:\n",
    "    all_names.add(t['head'])\n",
    "    all_names.add(t['tail'])\n",
    "\n",
    "print(f\"Recherche URIs pour {len(all_names)} entités...\")\n",
    "for nom in all_names:\n",
    "    uri = get_dbpedia_uri(nom)\n",
    "    if uri:\n",
    "        print(f\"{nom} -> {uri}\")\n",
    "        cache[nom] = URIRef(uri)\n",
    "    else:\n",
    "        # fallback\n",
    "        clean_nom = nom.replace(\" \", \"_\")\n",
    "        local_uri = EX[clean_nom]\n",
    "        print(f\"{nom} -> {local_uri} (Local)\")\n",
    "        cache[nom] = local_uri\n",
    "\n",
    "print(\"Ajout des triplets...\")\n",
    "for t in all_triplets:\n",
    "    s = cache[t['head']]\n",
    "    o = cache[t['tail']]\n",
    "    \n",
    "    # local relation\n",
    "    rel_clean = t['type'].replace(\" \", \"_\")\n",
    "    p = EX[rel_clean]\n",
    "    \n",
    "    g.add((s, p, o))\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" RÉSULTAT FINAL (.ttl) \")\n",
    "print(\"=\"*40)\n",
    "print(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a72e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test diff values for conf, support\n",
    "def test(entities_list):\n",
    "    scenarios = [\n",
    "        (0.3, 0),    \n",
    "        (0.5, 20),  \n",
    "        (0.8, 100)  \n",
    "    ]\n",
    "    sample_entities = list(set(entities_list))[:4] \n",
    "    \n",
    "    for ent in sample_entities:\n",
    "        for conf, supp in scenarios:\n",
    "            uri = get_dbpedia_uri(ent, conf, supp)\n",
    "            res = uri.split('/')[-1] if uri else None\n",
    "            print(f\"{ent}: {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96be1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph\n",
    "def build_final_graph(triplets):\n",
    "    g = Graph()\n",
    "    EX = Namespace(\"http://example.org/\") \n",
    "    \n",
    "    FINAL_CONF = 0.5\n",
    "    FINAL_SUPP = 20\n",
    "    \n",
    "    noms_uniques = set()\n",
    "    for t in triplets:\n",
    "        noms_uniques.add(t['head'])\n",
    "        noms_uniques.add(t['tail'])\n",
    "        \n",
    "    test(list(noms_uniques))\n",
    "        \n",
    "    uri_cache = {}\n",
    "    \n",
    "    for nom in noms_uniques:\n",
    "        uri = get_dbpedia_uri(nom, FINAL_CONF, FINAL_SUPP)\n",
    "        if uri:\n",
    "            uri_cache[nom] = URIRef(uri)\n",
    "        else:\n",
    "            clean = nom.replace(\" \", \"_\")\n",
    "            uri_cache[nom] = EX[clean]\n",
    "\n",
    "    for t in triplets:\n",
    "        s = uri_cache[t['head']]\n",
    "        o = uri_cache[t['tail']]\n",
    "        \n",
    "        rel_clean = t['type'].replace(\" \", \"_\").replace(\"-\", \"_\") # to get local relationships\n",
    "        p = EX[rel_clean]\n",
    "        \n",
    "        g.add((s, p, o))\n",
    "        \n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645ee29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boltzmann machine: http://dbpedia.org/resource/Boltzmann_machine\n",
      "Boltzmann machine: http://dbpedia.org/resource/Boltzmann_machine\n",
      "Boltzmann machine: None\n",
      "artificial neural network: http://dbpedia.org/resource/Artificial_neural_network\n",
      "artificial neural network: http://dbpedia.org/resource/Neural_network\n",
      "artificial neural network: http://dbpedia.org/resource/Neural_network\n",
      "Terry Sejnowski: http://dbpedia.org/resource/Terry_Sejnowski\n",
      "Terry Sejnowski: http://dbpedia.org/resource/Terry_Sejnowski\n",
      "Terry Sejnowski: None\n",
      "cognitive psychologist: http://dbpedia.org/resource/Cognitive_psychology\n",
      "cognitive psychologist: http://dbpedia.org/resource/Cognitive_psychology\n",
      "cognitive psychologist: http://dbpedia.org/resource/Cognitive_psychology\n"
     ]
    }
   ],
   "source": [
    "graph_final = build_final_graph(all_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2f124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@prefix ns1: <http://example.org/> .\n",
      "\n",
      "<http://dbpedia.org/resource/Boltzmann_machine> ns1:discoverer_or_inventor <http://dbpedia.org/resource/Terry_Sejnowski>,\n",
      "        ns1:David_Ackley .\n",
      "\n",
      "<http://dbpedia.org/resource/Christopher_Longuet-Higgins> ns1:employer <http://dbpedia.org/resource/Edinburgh> .\n",
      "\n",
      "<http://dbpedia.org/resource/David_Rumelhart> ns1:educated_at <http://dbpedia.org/resource/Carnegie_Mellon_University> .\n",
      "\n",
      "<http://dbpedia.org/resource/Mount_Everest> ns1:date_of_birth ns1:6_December_1947 ;\n",
      "    ns1:employer <http://dbpedia.org/resource/Toronto> ;\n",
      "    ns1:field_of_work <http://dbpedia.org/resource/Neural_network> ;\n",
      "    ns1:occupation <http://dbpedia.org/resource/Computer_scientist> .\n",
      "\n",
      "<http://dbpedia.org/resource/Open_access> ns1:subclass_of ns1:research_papers .\n",
      "\n",
      "<http://dbpedia.org/resource/Google> ns1:subsidiary <http://example.org/DNNresearch_Inc.> .\n",
      "\n",
      "<http://dbpedia.org/resource/Neural_network> ns1:part_of <http://dbpedia.org/resource/Cognitive_psychology> ;\n",
      "    ns1:use <http://dbpedia.org/resource/Machine_learning> .\n",
      "\n",
      "ns1:AlexNet ns1:discoverer_or_inventor ns1:Alex_Krizhevsky .\n",
      "\n",
      "ns1:Alex_Krizhevsky ns1:notable_work ns1:AlexNet .\n",
      "\n",
      "ns1:Brendan_Frey ns1:student_of ns1:Richard_Zemel .\n",
      "\n",
      "<http://example.org/DNNresearch_Inc.> ns1:parent_organization <http://dbpedia.org/resource/Google> .\n",
      "\n",
      "ns1:Richard_Zemel ns1:student ns1:Brendan_Frey .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(graph_final.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
